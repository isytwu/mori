diff --git a/examples/dist_rdma_ops/read.cpp b/examples/dist_rdma_ops/read.cpp
index a57a450..70fc205 100644
--- a/examples/dist_rdma_ops/read.cpp
+++ b/examples/dist_rdma_ops/read.cpp
@@ -102,7 +102,7 @@ __device__ void Quite(RdmaEndpoint* endpoint) {
             __hip_atomic_load(&cqHandle->consIdx, __ATOMIC_SEQ_CST, __HIP_MEMORY_SCOPE_AGENT);
       } while (completed != warp_cq_consumer);
       UpdateCqDbrRecord<core::ProviderType::MLX5>(cqHandle->dbrRecAddr,
-                                                  (uint32_t)(warp_cq_consumer + quiet_amount));
+                                                  (uint32_t)(warp_cq_consumer + quiet_amount));//告诉硬件哪些完成队列条目 (CQE) 已经被软件消费完毕，可以被硬件复用。
       __atomic_signal_fence(__ATOMIC_SEQ_CST);
 
       uint64_t doneIdx = wqe_broadcast[warp_id];
@@ -121,7 +121,7 @@ __global__ void Write(RdmaEndpoint* endpoint, MemoryRegion localMr, MemoryRegion
     uint8_t num_active_lanes = GetActiveLaneCount(activemask);
     uint8_t my_logical_lane_id = GetActiveLaneNum(activemask);
     bool is_leader{my_logical_lane_id == 0};
-    const uint64_t leader_phys_lane_id = GetFirstActiveLaneID(activemask);
+    const uint64_t leader_phys_lane_id = GetFirstActiveLaneID(activemask);  // my_logical_lane_id对应的物理id
     uint8_t num_wqes{num_active_lanes};
     uint64_t warp_sq_counter{0};
     WorkQueueHandle* wqHandle = &endpoint->wqHandle;
@@ -136,12 +136,12 @@ __global__ void Write(RdmaEndpoint* endpoint, MemoryRegion localMr, MemoryRegion
     uint64_t my_sq_index = my_sq_counter % wqHandle->sqWqeNum;
     while (true) {
       uint64_t db_touched =
-          __hip_atomic_load(&wqHandle->dbTouchIdx, __ATOMIC_SEQ_CST, __HIP_MEMORY_SCOPE_AGENT);
+          __hip_atomic_load(&wqHandle->dbTouchIdx, __ATOMIC_SEQ_CST, __HIP_MEMORY_SCOPE_AGENT);//ring dbr之后一次加num_wqes
       uint64_t db_done =
           __hip_atomic_load(&wqHandle->doneIdx, __ATOMIC_SEQ_CST, __HIP_MEMORY_SCOPE_AGENT);
       uint64_t num_active_sq_entries = db_touched - db_done;
       uint64_t num_free_entries = min(wqHandle->sqWqeNum, cqHandle->cqeNum) - num_active_sq_entries;
-      uint64_t num_entries_until_wave_last_entry = warp_sq_counter + num_active_lanes - db_touched;
+      uint64_t num_entries_until_wave_last_entry = warp_sq_counter + num_active_lanes - db_touched;//需要post的数量？
       if (num_free_entries > num_entries_until_wave_last_entry) {
         break;
       }
@@ -163,7 +163,7 @@ __global__ void Write(RdmaEndpoint* endpoint, MemoryRegion localMr, MemoryRegion
 
       uint8_t* base_ptr = reinterpret_cast<uint8_t*>(wqHandle->sqAddr);
       uint64_t* ctrl_wqe_8B_for_db = reinterpret_cast<uint64_t*>(
-          &base_ptr[64 * ((warp_sq_counter + num_wqes - 1) % wqHandle->sqWqeNum)]);
+          &base_ptr[64 * ((warp_sq_counter + num_wqes - 1) % wqHandle->sqWqeNum)]);//wqe slot的大小为64字节，后面是最后一个wqe的索引，得到最后一个wqe的绝对地址
       UpdateSendDbrRecord<ProviderType::MLX5>(wqHandle->dbrRecAddr, warp_sq_counter + num_wqes);
       // __threadfence_system();
       RingDoorbell<ProviderType::MLX5>(wqHandle->dbrAddr, *ctrl_wqe_8B_for_db);
@@ -210,7 +210,7 @@ void distRdmaOps(int argc, char* argv[]) {
   RdmaDevice* device = activeDevicePortList[local_rank % activeDevicePortList.size()].first;
   std::cout << "localRank " << local_rank << " select device " << device->Name() << std::endl;
 
-  RdmaDeviceContext* device_context = device->CreateRdmaDeviceContext();
+  RdmaDeviceContext* device_context = device->CreateRdmaDeviceContext();//创建PD
 
   // 2 Create an endpoint
   RdmaEndpointConfig config;
@@ -220,15 +220,15 @@ void distRdmaOps(int argc, char* argv[]) {
   config.maxCqeNum = 4096;
   config.alignment = 4096;
   config.onGpu = on_gpu;
-  RdmaEndpoint endpoint = device_context->CreateRdmaEndpoint(config);
+  RdmaEndpoint endpoint = device_context->CreateRdmaEndpoint(config);//创建QP CQ
 
-  // 3 Allgather global endpoint and connect
+  // 3 Allgather global endpoint and connect 交换端点信息
   std::vector<RdmaEndpointHandle> global_rdma_ep_handles(world_size);
   bootNet.Allgather(&endpoint.handle, global_rdma_ep_handles.data(), sizeof(RdmaEndpointHandle));
 
   std::cout << "Local rank " << local_rank << " " << endpoint.handle << std::endl;
 
-  for (int i = 0; i < world_size; i++) {
+  for (int i = 0; i < world_size; i++) {//TODO 这个怕是没有用，RC建链只能一对一
     if (i == local_rank) continue;
     device_context->ConnectEndpoint(endpoint.handle, global_rdma_ep_handles[i]);
     std::cout << "Local rank " << local_rank << " received " << global_rdma_ep_handles[i]
@@ -244,9 +244,9 @@ void distRdmaOps(int argc, char* argv[]) {
 
   // assert(!posix_memalign(&buffer_1, 4096, allreduce_size));
   // memset(buffer_1, 1, allreduce_size);
-  MemoryRegion mr_handle = device_context->RegisterMemoryRegion(buffer, totalSize, MR_ACCESS_FLAG);
+  MemoryRegion mr_handle = device_context->RegisterMemoryRegion(buffer, totalSize, MR_ACCESS_FLAG);//device_context包含一个pd
   std::vector<MemoryRegion> global_mr_handles(world_size);
-  bootNet.Allgather(&mr_handle, global_mr_handles.data(), sizeof(mr_handle));
+  bootNet.Allgather(&mr_handle, global_mr_handles.data(), sizeof(mr_handle));//交换内存区域信息
   global_mr_handles[local_rank] = mr_handle;
   RdmaEndpoint* devEndpoint;
   HIP_RUNTIME_CHECK(hipMalloc(&devEndpoint, sizeof(RdmaEndpoint)));
diff --git a/examples/ops/dispatch_combine/test_dispatch_combine.py b/examples/ops/dispatch_combine/test_dispatch_combine.py
index 4df69ba..8f1f2d6 100644
--- a/examples/ops/dispatch_combine/test_dispatch_combine.py
+++ b/examples/ops/dispatch_combine/test_dispatch_combine.py
@@ -223,10 +223,10 @@ class EpDispatchCombineTestCase:
             print("Dispatch Pass")
 
         total_recv_num_token = dispatch_recv_num_token[0].item()
-        combine_input = op.get_registered_input_buffer(self.config.data_type)
+        combine_input = op.get_registered_input_buffer(self.config.data_type)# GetRegisteredInputBuffer的实现就是从shmemInpTokMemObj获取一个tensor
         combine_input[:total_recv_num_token, :].copy_(
             dispatch_output[:total_recv_num_token, :]
-        )
+        )#手动拷贝了一下
 
         combine_input = dispatch_output
 
diff --git a/include/mori/application/transport/rdma/providers/mlx5/mlx5.hpp b/include/mori/application/transport/rdma/providers/mlx5/mlx5.hpp
index f30df9e..f9c0a23 100644
--- a/include/mori/application/transport/rdma/providers/mlx5/mlx5.hpp
+++ b/include/mori/application/transport/rdma/providers/mlx5/mlx5.hpp
@@ -54,13 +54,13 @@ class Mlx5CqContainer {
 };
 
 struct WorkQueueAttrs {
-  uint32_t wqeNum{0};
-  uint32_t wqeSize{0};
-  uint64_t wqSize{0};
-  uint32_t head{0};
-  uint32_t postIdx{0};
-  uint32_t wqeShift{0};
-  uint32_t offset{0};
+  uint32_t wqeNum{0};   // 工作队列元素数量
+  uint32_t wqeSize{0};  // 单个工作队列元素大小
+  uint64_t wqSize{0};  // 整个工作队列占用的内存字节数
+  uint32_t head{0};    // 队列头部索引：发送队列：指向下一个要被硬件处理的 WQE；接收队列：指向下一个可用的接收缓冲区
+  uint32_t postIdx{0};  // 提交索引：应用程序已经提交到队列的 WQE 数量
+  uint32_t wqeShift{0};  // wqeShift = log2(wqeSize)，方便位运算
+  uint32_t offset{0};    // 队列在整个内存区域中的字节偏移量
 };
 
 class Mlx5QpContainer {
diff --git a/include/mori/application/transport/rdma/rdma.hpp b/include/mori/application/transport/rdma/rdma.hpp
index 1faebb2..9e7d1b6 100644
--- a/include/mori/application/transport/rdma/rdma.hpp
+++ b/include/mori/application/transport/rdma/rdma.hpp
@@ -51,11 +51,11 @@ struct EthernetEndpointHandle {
 
 // TODO: add gid type
 struct RdmaEndpointHandle {
-  uint32_t psn{0};
-  uint32_t qpn{0};
-  uint32_t portId{0};
-  InfiniBandEndpointHandle ib;
-  EthernetEndpointHandle eth;
+  uint32_t psn{0};              // 包序列号
+  uint32_t qpn{0};              // 队列对编号
+  uint32_t portId{0};           // 端口ID
+  InfiniBandEndpointHandle ib;  // InfiniBand 特定信息
+  EthernetEndpointHandle eth;   // 以太网特定信息
 };
 
 struct WorkQueueHandle {
@@ -87,7 +87,7 @@ struct CompletionQueueHandle {
 
 struct RdmaEndpoint {
   RdmaDeviceVendorId vendorId{RdmaDeviceVendorId::Unknown};
-  RdmaEndpointHandle handle;
+  RdmaEndpointHandle handle;//allgather获取所有rank的handle
   WorkQueueHandle wqHandle;
   CompletionQueueHandle cqHandle;
 
diff --git a/include/mori/core/transport/rdma/providers/mlx5/mlx5_device_primitives.hpp b/include/mori/core/transport/rdma/providers/mlx5/mlx5_device_primitives.hpp
index 99be281..a31bed1 100644
--- a/include/mori/core/transport/rdma/providers/mlx5/mlx5_device_primitives.hpp
+++ b/include/mori/core/transport/rdma/providers/mlx5/mlx5_device_primitives.hpp
@@ -60,7 +60,7 @@ inline __device__ void PostRecv<ProviderType::MLX5>(void* queueBuffAddr, uint32_
 /*                                        Read / Write APIs                                       */
 /* ---------------------------------------------------------------------------------------------- */
 static constexpr int SendWqeSize =
-    sizeof(mlx5_wqe_ctrl_seg) + sizeof(mlx5_wqe_raddr_seg) + sizeof(mlx5_wqe_data_seg);
+    sizeof(mlx5_wqe_ctrl_seg) + sizeof(mlx5_wqe_raddr_seg) + sizeof(mlx5_wqe_data_seg);//分别为PostReadWrite填写的三个字段
 static constexpr int SendWqeNumOctoWords = CeilDiv(SendWqeSize, 16);
 static constexpr int SendWqeNumWqeBb = CeilDiv(SendWqeNumOctoWords * 16, int(MLX5_SEND_WQE_BB));
 
@@ -75,9 +75,9 @@ inline __device__ uint64_t PostReadWrite(void* queueBuffAddr, uint32_t wqeNum, u
   uintptr_t wqeAddr = reinterpret_cast<uintptr_t>(queueBuffAddr) + (wqeIdx << MLX5_SEND_WQE_SHIFT);
 
   mlx5_wqe_ctrl_seg* wqeCtrlSeg = reinterpret_cast<mlx5_wqe_ctrl_seg*>(wqeAddr);
-  wqeCtrlSeg->opmod_idx_opcode = HTOBE32(((curPostIdx & 0xffff) << 8) | opcode);
-  wqeCtrlSeg->qpn_ds = HTOBE32((qpn << 8) | SendWqeNumOctoWords);
-  wqeCtrlSeg->fm_ce_se = MLX5_WQE_CTRL_CQ_UPDATE;
+  wqeCtrlSeg->opmod_idx_opcode = HTOBE32(((curPostIdx & 0xffff) << 8) | opcode);//[31:16]操作修饰符, [15:8]WQE索引, [7:0]操作码
+  wqeCtrlSeg->qpn_ds = HTOBE32((qpn << 8) | SendWqeNumOctoWords);//[31:8]Queue Pair号, [7:0]数据段数量（以16字节为单位）
+  wqeCtrlSeg->fm_ce_se = MLX5_WQE_CTRL_CQ_UPDATE;//控制标志，MLX5_WQE_CTRL_CQ_UPDATE 表示完成时生成 CQE
 
   mlx5_wqe_raddr_seg* wqeRaddrSeg =
       reinterpret_cast<mlx5_wqe_raddr_seg*>(wqeAddr + sizeof(mlx5_wqe_ctrl_seg));
@@ -90,7 +90,7 @@ inline __device__ uint64_t PostReadWrite(void* queueBuffAddr, uint32_t wqeNum, u
   wqeDataSeg->addr = HTOBE64(laddr);
   wqeDataSeg->lkey = HTOBE32(lkey);
 
-  return reinterpret_cast<uint64_t*>(wqeCtrlSeg)[0];
+  return reinterpret_cast<uint64_t*>(wqeCtrlSeg)[0];//返回控制段的前 8 字节作为 doorbell 值
 }
 
 template <>
@@ -295,7 +295,7 @@ inline __device__ uint64_t mlx5PrepareAtomicWqe(void* queue_buff_addr, uint32_t
       reinterpret_cast<char*>(queue_buff_addr) + ((wqeIdx + 1) << MLX5_SEND_WQE_SHIFT);
 
   int atomicWqeSize =
-      sizeof(mlx5_wqe_ctrl_seg) + sizeof(mlx5_wqe_raddr_seg) + 2 * sizeof(mlx5_wqe_atomic_seg);
+      sizeof(mlx5_wqe_ctrl_seg) + sizeof(mlx5_wqe_raddr_seg) + 2 * sizeof(mlx5_wqe_atomic_seg);//2个？
 
   struct mlx5_wqe_ctrl_seg* wqeCtrlSeg = reinterpret_cast<mlx5_wqe_ctrl_seg*>(wqeAddr);
   struct mlx5_wqe_raddr_seg* wqeRaddrSeg = reinterpret_cast<mlx5_wqe_raddr_seg*>(
@@ -570,7 +570,7 @@ DEFINE_POST_ATOMIC_SPEC(int64_t)
 template <>
 inline __device__ void UpdateSendDbrRecord<ProviderType::MLX5>(void* dbrRecAddr, uint32_t wqeIdx) {
   core::AtomicStoreSeqCstSystem(reinterpret_cast<uint32_t*>(dbrRecAddr) + MLX5_SND_DBR,
-                                HTOBE32(wqeIdx & 0xffff));
+                                HTOBE32(wqeIdx & 0xffff));//取16位，Door Bell Record 告诉硬件："处理到该索引为止的所有 WQE"
 }
 
 template <>
@@ -581,7 +581,7 @@ inline __device__ void UpdateRecvDbrRecord<ProviderType::MLX5>(void* dbrRecAddr,
 
 template <>
 inline __device__ void RingDoorbell<ProviderType::MLX5>(void* dbrAddr, uint64_t dbrVal) {
-  core::AtomicStoreSeqCstSystem(reinterpret_cast<uint64_t*>(dbrAddr), dbrVal);
+  core::AtomicStoreSeqCstSystem(reinterpret_cast<uint64_t*>(dbrAddr), dbrVal);//真正触发硬件开始处理 WQE 的操作
 }
 
 template <>
@@ -620,8 +620,8 @@ inline __device__ int PollCqOnce<ProviderType::MLX5>(void* cqeAddr, uint32_t cqe
   auto* lastBytePtr = reinterpret_cast<uint8_t*>(cqeQwords + 7) + 7;
   uint8_t opOwn = *lastBytePtr;
 
-  uint8_t opcode = opOwn >> 4;
-  uint8_t owner = opOwn & MLX5_CQE_OWNER_MASK;
+  uint8_t opcode = opOwn >> 4;                  // 高 4 位：操作码
+  uint8_t owner = opOwn & MLX5_CQE_OWNER_MASK;  // 低 1 位：owner bit
 
   bool is_empty = true;
   for (int i = 0; i < (sizeof(mlx5_cqe64) / sizeof(uint64_t)); i++) {
@@ -629,7 +629,7 @@ inline __device__ int PollCqOnce<ProviderType::MLX5>(void* cqeAddr, uint32_t cqe
       is_empty = false;
       break;
     }
-  }
+  }//空字节检查，避免硬件没有完全写入CQE
 
   // TODO: check if cqeNum should be power of 2?
   //   int cq_owner_flip = !!(consIdx & (cqeNum + 1));
@@ -638,7 +638,7 @@ inline __device__ int PollCqOnce<ProviderType::MLX5>(void* cqeAddr, uint32_t cqe
     return -1;
   }
 
-  *lastBytePtr = (MLX5_CQE_INVALID << 4) | (cq_owner_flip & 1);
+  *lastBytePtr = (MLX5_CQE_INVALID << 4) | (cq_owner_flip & 1);  // 标记 CQE 为已处理
   return opcode;
 }
 
@@ -682,7 +682,7 @@ inline __device__ int PollCq<ProviderType::MLX5>(void* cqAddr, uint32_t cqeNum,
     printf("(%s:%d) CQE error: %s\n", __FILE__, __LINE__, IbvWcStatusString(error));
     return opcode;
   }
-  *wqeCounter = BE16TOH(reinterpret_cast<mlx5_cqe64*>(cqeAddr)->wqe_counter);
+  *wqeCounter = wqeCounter(reinterpret_cast<mlx5_cqe64*>(cqeAddr)->wqe_counter);//硬件在完成 WQE 后，在 CQE 中返回的一个标识符，对应于原始 WQE 提交时的计数器值
   return opcode;
 }
 
diff --git a/src/application/transport/rdma/providers/mlx5/mlx5.cpp b/src/application/transport/rdma/providers/mlx5/mlx5.cpp
index 9b14345..5aacd1a 100644
--- a/src/application/transport/rdma/providers/mlx5/mlx5.cpp
+++ b/src/application/transport/rdma/providers/mlx5/mlx5.cpp
@@ -52,7 +52,7 @@ Mlx5CqContainer::Mlx5CqContainer(ibv_context* context, const RdmaEndpointConfig&
   int status;
   uint8_t cmd_in[DEVX_ST_SZ_BYTES(create_cq_in)] = {
       0,
-  };
+  };  // DEVX_ST_SZ_BYTES：device exteneded interface；structure；size；bytes计算结构体create_cq_in的字节数
   uint8_t cmd_out[DEVX_ST_SZ_BYTES(create_cq_out)] = {
       0,
   };
@@ -73,7 +73,7 @@ Mlx5CqContainer::Mlx5CqContainer(ibv_context* context, const RdmaEndpointConfig&
     assert(!status);
   }
 
-  cqUmem = mlx5dv_devx_umem_reg(context, cqUmemAddr, cqSize, IBV_ACCESS_LOCAL_WRITE);
+  cqUmem = mlx5dv_devx_umem_reg(context, cqUmemAddr, cqSize, IBV_ACCESS_LOCAL_WRITE);//硬件可以直接访问这块内存来写入 CQE
   assert(cqUmem);
 
   // Allocate user memory for CQ DBR (doorbell?)
@@ -86,15 +86,15 @@ Mlx5CqContainer::Mlx5CqContainer(ibv_context* context, const RdmaEndpointConfig&
     assert(!status);
   }
 
-  cqDbrUmem = mlx5dv_devx_umem_reg(context, cqDbrUmemAddr, 8, IBV_ACCESS_LOCAL_WRITE);
+  cqDbrUmem = mlx5dv_devx_umem_reg(context, cqDbrUmemAddr, 8, IBV_ACCESS_LOCAL_WRITE);//记录当前消费的 CQE 索引,硬件通过这个记录知道哪些 CQE 已经被处理
   assert(cqDbrUmem);
 
   // Allocate user access region
-  uar = mlx5dv_devx_alloc_uar(context, MLX5DV_UAR_ALLOC_TYPE_NC);
+  uar = mlx5dv_devx_alloc_uar(context, MLX5DV_UAR_ALLOC_TYPE_NC);//用户空间可以直接访问的硬件寄存器页面,Non-Cached 类型，确保写入立即到达硬件
   assert(uar->page_id != 0);
 
   // Initialize CQ
-  DEVX_SET(create_cq_in, cmd_in, opcode, MLX5_CMD_OP_CREATE_CQ);
+  DEVX_SET(create_cq_in, cmd_in, opcode, MLX5_CMD_OP_CREATE_CQ);//DecX接口Device eXtended interface
   DEVX_SET(create_cq_in, cmd_in, cq_umem_valid, 0x1);
   DEVX_SET(create_cq_in, cmd_in, cq_umem_id, cqUmem->umem_id);
 
@@ -104,7 +104,7 @@ Mlx5CqContainer::Mlx5CqContainer(ibv_context* context, const RdmaEndpointConfig&
   DEVX_SET(cqc, cq_context, log_cq_size, LogCeil2(cqeNum));
   DEVX_SET(cqc, cq_context, uar_page, uar->page_id);
 
-  uint32_t eqn;
+  uint32_t eqn;//事件队列编号
   status = mlx5dv_devx_query_eqn(context, 0, &eqn);
   assert(!status);
   DEVX_SET(cqc, cq_context, c_eqn, eqn);
@@ -144,8 +144,8 @@ void Mlx5QpContainer::ComputeQueueAttrs(const RdmaEndpointConfig& config) {
   rqAttrs.offset = 0;
 
   // Send queue attributes
-  sqAttrs.offset = rqAttrs.wqSize;
-  sqAttrs.wqeSize = GetMlx5SqWqeSize();
+  sqAttrs.offset = rqAttrs.wqSize;//sq在rq后面？
+  sqAttrs.wqeSize = GetMlx5SqWqeSize();//16B
   sqAttrs.wqSize = RoundUpPowOfTwo(sqAttrs.wqeSize * config.maxMsgsNum);
   sqAttrs.wqeNum = ceil(sqAttrs.wqSize / MLX5_SEND_WQE_BB);
   sqAttrs.wqeShift = MLX5_SEND_WQE_SHIFT;
@@ -256,8 +256,8 @@ void Mlx5QpContainer::DestroyQueuePair() {
 void* Mlx5QpContainer::GetSqAddress() { return static_cast<char*>(qpUmemAddr) + sqAttrs.offset; }
 
 void* Mlx5QpContainer::GetRqAddress() { return static_cast<char*>(qpUmemAddr) + rqAttrs.offset; }
-
-void Mlx5QpContainer::ModifyRst2Init() {
+// RESET → INIT → RTR (Ready to Receive) → RTS (Ready to Send) → SQD → ERROR
+void Mlx5QpContainer::ModifyRst2Init() {//基本权限和端口配置
   uint8_t rst2init_cmd_in[DEVX_ST_SZ_BYTES(rst2init_qp_in)] = {
       0,
   };
@@ -271,8 +271,8 @@ void Mlx5QpContainer::ModifyRst2Init() {
   void* qpc = DEVX_ADDR_OF(rst2init_qp_in, rst2init_cmd_in, qpc);
   DEVX_SET(qpc, qpc, rwe, 1); /* remote write access */
   DEVX_SET(qpc, qpc, rre, 1); /* remote read access */
-  DEVX_SET(qpc, qpc, rae, 1); 
-  DEVX_SET(qpc, qpc, atomic_mode, 0x3);
+  DEVX_SET(qpc, qpc, rae, 1);
+  DEVX_SET(qpc, qpc, atomic_mode, 0x3);  // 0x3 = 支持所有原子操作
   DEVX_SET(qpc, qpc, primary_address_path.vhca_port_num, config.portId);
 
   DEVX_SET(qpc, qpc, pm_state, 0x3);
@@ -284,7 +284,7 @@ void Mlx5QpContainer::ModifyRst2Init() {
 }
 
 void Mlx5QpContainer::ModifyInit2Rtr(const RdmaEndpointHandle& remote_handle,
-                                     const ibv_port_attr& portAttr) {
+                                     const ibv_port_attr& portAttr) {//配置远程端点信息
   uint8_t init2rtr_cmd_in[DEVX_ST_SZ_BYTES(init2rtr_qp_in)] = {
       0,
   };
@@ -296,12 +296,12 @@ void Mlx5QpContainer::ModifyInit2Rtr(const RdmaEndpointHandle& remote_handle,
   DEVX_SET(init2rtr_qp_in, init2rtr_cmd_in, qpn, qpn);
 
   void* qpc = DEVX_ADDR_OF(init2rtr_qp_in, init2rtr_cmd_in, qpc);
-  DEVX_SET(qpc, qpc, mtu, portAttr.active_mtu);
-  DEVX_SET(qpc, qpc, log_msg_max, 30);
-  DEVX_SET(qpc, qpc, remote_qpn, remote_handle.qpn);
-  DEVX_SET(qpc, qpc, next_rcv_psn, remote_handle.psn);
-  DEVX_SET(qpc, qpc, min_rnr_nak, 12);
-  DEVX_SET(qpc, qpc, log_rra_max, 20);
+  DEVX_SET(qpc, qpc, mtu, portAttr.active_mtu);         // 最大传输单元
+  DEVX_SET(qpc, qpc, log_msg_max, 30);                  // 最大消息大小（2^30）
+  DEVX_SET(qpc, qpc, remote_qpn, remote_handle.qpn);    // 远程QP编号
+  DEVX_SET(qpc, qpc, next_rcv_psn, remote_handle.psn);  // 期望接收的包序列号
+  DEVX_SET(qpc, qpc, min_rnr_nak, 12);                  // RNR NAK 最小延迟(Receiver Not Ready;Negative Acknowledgment 队列满时触发，收到NAK之后等待的时长)
+  DEVX_SET(qpc, qpc, log_rra_max, 20);                  // 最大读取响应数
 
   qpc = DEVX_ADDR_OF(init2rtr_qp_in, init2rtr_cmd_in, qpc);
   DEVX_SET(qpc, qpc, primary_address_path.vhca_port_num, config.portId);
@@ -313,9 +313,9 @@ void Mlx5QpContainer::ModifyInit2Rtr(const RdmaEndpointHandle& remote_handle,
 
     memcpy(DEVX_ADDR_OF(qpc, qpc, primary_address_path.rmac_47_32), remote_handle.eth.mac,
            sizeof(remote_handle.eth.mac));
-    DEVX_SET(qpc, qpc, primary_address_path.hop_limit, 64);
+    DEVX_SET(qpc, qpc, primary_address_path.hop_limit, 64);//TTL
     DEVX_SET(qpc, qpc, primary_address_path.src_addr_index, config.gidIdx);
-    DEVX_SET(qpc, qpc, primary_address_path.udp_sport, 0xC000);
+    DEVX_SET(qpc, qpc, primary_address_path.udp_sport, 0xC000);  // UDP源端口
   } else if (portAttr.link_layer == IBV_LINK_LAYER_INFINIBAND) {
     DEVX_SET(qpc, qpc, primary_address_path.rlid, remote_handle.ib.lid);
   } else {
@@ -339,11 +339,11 @@ void Mlx5QpContainer::ModifyRtr2Rts(const RdmaEndpointHandle& local_handle) {
   DEVX_SET(rtr2rts_qp_in, rtr2rts_cmd_in, qpn, qpn);
 
   void* qpc = DEVX_ADDR_OF(rtr2rts_qp_in, rtr2rts_cmd_in, qpc);
-  DEVX_SET(qpc, qpc, log_sra_max, 20);
-  DEVX_SET(qpc, qpc, next_send_psn, local_handle.psn);
-  DEVX_SET(qpc, qpc, retry_count, 7);
-  DEVX_SET(qpc, qpc, rnr_retry, 7);
-  DEVX_SET(qpc, qpc, primary_address_path.ack_timeout, 20);
+  DEVX_SET(qpc, qpc, log_sra_max, 20);                       // 最大发送请求数
+  DEVX_SET(qpc, qpc, next_send_psn, local_handle.psn);       // 下一个发送包序列号
+  DEVX_SET(qpc, qpc, retry_count, 7);                        // 重试次数
+  DEVX_SET(qpc, qpc, rnr_retry, 7);                          // RNR 重试次数
+  DEVX_SET(qpc, qpc, primary_address_path.ack_timeout, 20);  // ACK 超时
   DEVX_SET(qpc, qpc, primary_address_path.vhca_port_num, config.portId);
 
   int status = mlx5dv_devx_obj_modify(qp, rtr2rts_cmd_in, sizeof(rtr2rts_cmd_in), rtr2rts_cmd_out,
@@ -365,7 +365,7 @@ Mlx5DeviceContext::Mlx5DeviceContext(RdmaDevice* rdma_device, ibv_pd* in_pd)
   dv_obj.pd.out = &dvpd;
   int status = mlx5dv_init_obj(&dv_obj, MLX5DV_OBJ_PD);
   assert(!status);
-  pdn = dvpd.pdn;
+  pdn = dvpd.pdn;//这里专门用dv verbs接口获取了pdn？
 }
 
 Mlx5DeviceContext::~Mlx5DeviceContext() {}
@@ -459,7 +459,7 @@ Mlx5Device::~Mlx5Device() {}
 
 RdmaDeviceContext* Mlx5Device::CreateRdmaDeviceContext() {
   ibv_pd* pd = ibv_alloc_pd(defaultContext);
-  return new Mlx5DeviceContext(this, pd);
+  return new Mlx5DeviceContext(this, pd);//构造 获取了pdn
 }
 
 }  // namespace application
diff --git a/src/ops/dispatch_combine/internode.hpp b/src/ops/dispatch_combine/internode.hpp
index 5eef8e0..2491bb2 100644
--- a/src/ops/dispatch_combine/internode.hpp
+++ b/src/ops/dispatch_combine/internode.hpp
@@ -21,7 +21,7 @@ __device__ void SyncIfDebugEnabled(const char* msg) {
   __syncthreads();
 #endif
 }
-
+//TODO 优化：少用block性能相同；atomic换imm？
 /* ---------------------------------------------------------------------------------------------- */
 /*                                    EpDispatchInterNodeKernel                                   */
 /* ---------------------------------------------------------------------------------------------- */
@@ -59,7 +59,7 @@ __global__ void EpDispatchInterNodeKernel(EpDispatchCombineArgs<T> args) {
   
   extern __shared__ char sharedMem[];
 
-  int subWarpNumPerWarp = warpSize / numExpertPerToken;
+  int subWarpNumPerWarp = warpSize / numExpertPerToken;//32/topk，每个EP对应一个subwarp
   int laneInSubWarp = laneId % numExpertPerToken;
   int subWarpId = laneId / numExpertPerToken;
   int globalSubWarpId = globalWarpId * subWarpNumPerWarp + subWarpId;
@@ -84,14 +84,14 @@ __global__ void EpDispatchInterNodeKernel(EpDispatchCombineArgs<T> args) {
         continue;
       } else {
         index_t destPeTokenIdx = 0, peSortedIdx = 0;
-        destPeTokenIdx = atomicAdd(args.destPeTokenCounter + destPe, 1);
-        peSortedIdx = destPe * MaxNumTokensToRecvPerRank + destPeTokenIdx;
+        destPeTokenIdx = atomicAdd(args.destPeTokenCounter + destPe, 1);//和intranode不同，这里不是所有pe对于destPE取号
+        peSortedIdx = destPe * MaxNumTokensToRecvPerRank + destPeTokenIdx;//写入staging的位置
         args.dispSenderIdxMap[expertOffset] = peSortedIdx;
         args.destPeTokenIdxMap[peSortedIdx] = tokenId;
         __threadfence();
       }
     }
-  }
+  }  // 优化了去重逻辑，增加了subwarp，另外用了__match_any_sync；已经计算好了发给每个PE多少token-destPeTokenCounter，以及位置
 
   if (laneId == 0) {
     int old_val = atomicAdd(args.dispatchGridBarrier, 1);
@@ -102,14 +102,14 @@ __global__ void EpDispatchInterNodeKernel(EpDispatchCombineArgs<T> args) {
 
   if (laneId == 0) {
     shmem::ShmemUint32WaitUntilEquals(args.dispatchGridBarrier, 0);
-  }
+  }//所有warp的同步，TODO 会有bug：99行很多warp都在置0，连续做同步，其他进去下一轮的warp加1会被置0
 
   // TODO: block num should be multiple of npes
   const int numsBlockPerDestPe = gridDim.x / npes;
   const int destPe = blockIdx.x / numsBlockPerDestPe;
   const int destNode = destPe / MAX_GPUS_PER_NODE;
   const int localBlockId = blockIdx.x - destPe * numsBlockPerDestPe;
-  const int totalTokens = args.destPeTokenCounter[destPe];
+  const int totalTokens = args.destPeTokenCounter[destPe];//要发给destPe的总token
   const int baseChunk = totalTokens / numsBlockPerDestPe;
   const int remainder = totalTokens % numsBlockPerDestPe;
 
@@ -159,15 +159,15 @@ __global__ void EpDispatchInterNodeKernel(EpDispatchCombineArgs<T> args) {
       gatherTokenNum[idx] = 0;
     }
     __syncthreads();
-    const int chunkTokenSize = (warpNum - 1);
-    if (warpId == warpNum - 1) {
+    const int chunkTokenSize = (warpNum - 1);//要聚合的token数
+    if (warpId == warpNum - 1) {//调用ibgda接口
       const int totalTokenInBlock = endIdx - startIdx;
       const int totalChunk = totalTokenInBlock / chunkTokenSize;
       const int remainTokenNum = totalTokenInBlock % chunkTokenSize;
       for (int chunkIdx = 0; chunkIdx < totalChunk; chunkIdx++) {
         if (laneId == 0) {
           while (atomicAdd(&gatherTokenNum[chunkIdx], 0) < chunkTokenSize) {
-            ;
+            ;//aggregate 所有warp的token
           }
         }
         // rdma_send
@@ -193,15 +193,15 @@ __global__ void EpDispatchInterNodeKernel(EpDispatchCombineArgs<T> args) {
         // rdma_send
         const index_t srcIdx =
             destPe * MaxNumTokensToRecvPerRank + startIdx + totalChunk * chunkTokenSize;
-        size_t srcOffset = srcIdx * stagingOffset;
+        size_t srcOffset = srcIdx * stagingOffset
         const index_t dstIdx =
             myPe * MaxNumTokensToRecvPerRank + startIdx + totalChunk * chunkTokenSize;
         size_t dstOffset = dstIdx * stagingOffset;
         shmem::ShmemPutTypeNbiWarp<uint8_t>(args.shmemInpTokMemObj, dstOffset,
                                             args.shmemStagingTokMemObj, srcOffset,
-                                            remainTokenNum * stagingOffset, destPe);
+                                            remainTokenNum * stagingOffset, destPe);//有点奇怪，rdma写道对面的shmemInpTokMemObj？？？有点滥用symm buffer了
       }
-    } else {
+    } else {//负责拷贝到staging
       // int warpTokens = 0;
       int chunkIdx = 0;
       for (int idx = warpId; idx < endIdx - startIdx; idx += chunkTokenSize) {
@@ -225,7 +225,7 @@ __global__ void EpDispatchInterNodeKernel(EpDispatchCombineArgs<T> args) {
               reinterpret_cast<char*>(args.scalesBuf) + tokenId * config.scaleDim * config.scaleTypeSize,
               config.scaleDim * config.scaleTypeSize);
         }
-        if (laneId == 0) atomicAdd(&gatherTokenNum[chunkIdx++], 1);
+        if (laneId == 0) atomicAdd(&gatherTokenNum[chunkIdx++], 1);//通知协调warp，一个token已经拷入staging
       }
       // if (laneId == 0 && warpTokens) atomicAdd(&gatherTokenNum, warpTokens);
       __threadfence_block();
@@ -239,10 +239,10 @@ __global__ void EpDispatchInterNodeKernel(EpDispatchCombineArgs<T> args) {
     //                                         args.shmemStagingTokMemObj->GetMemoryRegion(myPe), myPe * sizeof(index_t),
     //                                         (int32_t)(totalTokens+1), destPe, core::AMO_SET);
     int doneBlockNum = atomicAdd(&args.dispatchGridBarrier[destPe], 1);
-    if (doneBlockNum == numsBlockPerDestPe - 1) 
+    if (doneBlockNum == numsBlockPerDestPe - 1)  // numsBlockPerDestPe个block处理发到同一个destPe，这几个block的同步保证destPe都发完了
     {
       shmem::ShmemPutInt32ImmNbiThread(args.recvTokenNumMemObj, myPe * sizeof(index_t),
-                                    totalTokens + 1, destPe);
+                                    totalTokens + 1, destPe);//recvTokenNumMemObj也是个symm buffer
       __hip_atomic_store(&args.dispatchGridBarrier[destPe], 0, __ATOMIC_RELAXED, __HIP_MEMORY_SCOPE_AGENT);
     }
   }
@@ -258,7 +258,7 @@ __global__ void EpDispatchInterNodeKernel(EpDispatchCombineArgs<T> args) {
   }
   __syncthreads();
 
-  const int baseRecvChunk = recvTokenNum / numsBlockPerDestPe;
+  const int baseRecvChunk = recvTokenNum / numsBlockPerDestPe;//这里recvTokenNum在sh mem上
   const int recvRemainder = recvTokenNum % numsBlockPerDestPe;
   const int myRecvChunkSize = baseRecvChunk + (localBlockId < recvRemainder);
   // if (localBlockId == 0 && thdId == 0) printf("rank[%d] destPe[%d] myRecvChunkSize: %d\n", myPe, destPe, myRecvChunkSize);
@@ -267,7 +267,7 @@ __global__ void EpDispatchInterNodeKernel(EpDispatchCombineArgs<T> args) {
   for (int idx = warpId; idx < myRecvChunkSize; idx += warpNum) {
     index_t localTokenIdx = 0;
     if (laneId == 0) {
-      localTokenIdx = atomicAdd(args.localPeTokenCounter, 1);
+      localTokenIdx = atomicAdd(args.localPeTokenCounter, 1);//这里取号又会导致乱序
     }
     localTokenIdx = __shfl(localTokenIdx, 0);
     index_t peSortedId = destPe * MaxNumTokensToRecvPerRank + startRecvIdx + idx;
@@ -364,14 +364,14 @@ __global__ void EpCombineInterNodeKernel(EpDispatchCombineArgs<T> args) {
   // This phase is symmetric with dispatch recv phase, where tokens are first sent back to its
   // source pe in pe sorted order
   const int numsBlockPerSrcPe = gridDim.x / npes;
-  const int srcPe = blockIdx.x / numsBlockPerSrcPe;
+  const int srcPe = blockIdx.x / numsBlockPerSrcPe;//几个block处理一个pe
   const int srcNode = srcPe / MAX_GPUS_PER_NODE;
   const int localBlockId = blockIdx.x - srcPe * numsBlockPerSrcPe;
   const int srcPeTokenNum = *(args.recvTokenNumMemObj->template GetAs<index_t*>() + srcPe) - 1;
-  const int baseChunk = srcPeTokenNum / numsBlockPerSrcPe;
-  const int remainder = srcPeTokenNum % numsBlockPerSrcPe;
+  const int baseChunk = srcPeTokenNum / numsBlockPerSrcPe;//token再均分给block，应该叫baseChunkToken
+  const int remainder = srcPeTokenNum % numsBlockPerSrcPe;//剩下的token？reminder什么鬼命名
 
-  const int myChunkSize = baseChunk + (localBlockId < remainder);
+  const int myChunkSize = baseChunk + (localBlockId < remainder);//numTokenPerBlock
 
   const int startIdx = localBlockId * baseChunk + min(localBlockId, remainder);
   const int endIdx = startIdx + myChunkSize;
@@ -380,7 +380,7 @@ __global__ void EpCombineInterNodeKernel(EpDispatchCombineArgs<T> args) {
     // intra node use xgmi for transfer
     for (int idx = warpId; idx < endIdx - startIdx; idx += warpNum) {
       const index_t mapIdx = srcPe * MaxNumTokensToRecvPerRank + startIdx + idx;
-      size_t mapIdxOffset = mapIdx * size_t(config.hiddenDim);
+      size_t mapIdxOffset = mapIdx * size_t(config.hiddenDim);//stagingoffset更贴切
       const index_t tokenId = args.srcPeTokenIdxMap[mapIdx];
       size_t tokenOffset = tokenId * size_t(config.hiddenDim);
       const index_t peSortedId = myPe * MaxNumTokensToRecvPerRank + startIdx + idx;
diff --git a/src/ops/dispatch_combine/intranode.hpp b/src/ops/dispatch_combine/intranode.hpp
index cedf7a8..664f538 100644
--- a/src/ops/dispatch_combine/intranode.hpp
+++ b/src/ops/dispatch_combine/intranode.hpp
@@ -88,13 +88,13 @@ __global__ void EpDispatchIntraNodeKernel(EpDispatchCombineArgs<T> args) {
 
       if (laneId == 0) {
         // decide token id in dest pe
-        destTokId = atomicAdd(args.dispTokOffsetMemObj->template GetAs<index_t*>(destPe), 1);
-        atomicAdd(args.destPeTokenCounter + destPe, 1);
-        args.dispDestTokIdMap[i] = destPe * maxNumOutTokenPerRank + destTokId;
+        destTokId = atomicAdd(args.dispTokOffsetMemObj->template GetAs<index_t*>(destPe), 1);//在dest那边取号，所有pe都用这个取号？dispTokOffsetMemObj应该是全pe可见的buffer，而不是一对一的buffer
+        atomicAdd(args.destPeTokenCounter + destPe, 1);//本地计数：发给destP的token总数
+        args.dispDestTokIdMap[i] = destPe * maxNumOutTokenPerRank + destTokId;//topk_idx的序号 与 发到dest的序号信息的映射,这个值不是实际的存储位置
 
         // TODO: use a switch to control the writing of this buffer, should only turn on for testing
         args.dispTokIdToSrcTokIdMemObj->template GetAs<index_t*>(destPe)[destTokId] =
-            myPe * config.maxNumInpTokenPerRank + srcTokId;
+            myPe * config.maxNumInpTokenPerRank + srcTokId;//因为收到的token是pe随意排序的：pe0t1，pe3t0，pe2t2...提供给用户src token信息
       }
       destTokId = __shfl(destTokId, 0);
 
@@ -120,7 +120,7 @@ __global__ void EpDispatchIntraNodeKernel(EpDispatchCombineArgs<T> args) {
       index_t srcTokOffset = srcTokId * config.hiddenDim;
       index_t destTokOffset = destTokId * config.hiddenDim;
       core::WarpCopy(args.shmemOutTokMemObj->template GetAs<T*>(destPe) + destTokOffset,
-                     args.inpTokenBuf + srcTokOffset, config.hiddenDim);
+                     args.inpTokenBuf + srcTokOffset, config.hiddenDim);//来自不同PE的token根据取号顺序写入
     }
   }
   if (laneId == 0) atomicAdd(args.dispatchGridBarrier, 1);
@@ -225,7 +225,7 @@ __global__ void EpCombineIntraNodeKernel(EpDispatchCombineArgs<T> args) {
     }
     core::WarpAccum<T, 8>(
         args.shmemOutTokMemObj->template GetAs<T*>() + tokenId * config.hiddenDim + hiddenDimOffset,
-        srcPtrs, nullptr, config.numExpertPerToken, hiddenDimSize);
+        srcPtrs, nullptr, config.numExpertPerToken, hiddenDimSize);//srcPtrs都在别的卡上，这里是拉数据，到本地做reduce
   }
 }
 
